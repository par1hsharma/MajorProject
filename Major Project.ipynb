{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e19af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e065b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_typical_prof(df_train, prof_ind):\n",
    "    profession = df_train[\"Profession\"].values\n",
    "    prof_inds = np.where(profession==prof_ind)[0]\n",
    "    \n",
    "    # Create aggregated results for each feature\n",
    "    gender = Counter(df_train[\"Gender\"].values[prof_inds]).most_common(1)[0][0]\n",
    "    ever_married = Counter(df_train[\"Ever_Married\"].values[prof_inds]).most_common(1)[0][0]\n",
    "    age = [i for i in df_train[\"Age\"].values[prof_inds] if i!=-999]\n",
    "    graduated = Counter(df_train[\"Graduated\"].values[prof_inds]).most_common(1)[0][0]\n",
    "    work_exp = [i for i in df_train[\"Work_Experience\"].values[prof_inds] if i!=-999]\n",
    "    spend_score = Counter(df_train[\"Spending_Score\"].values[prof_inds]).most_common(1)[0][0]\n",
    "    family_size = [i for i in df_train[\"Family_Size\"].values[prof_inds] if i!=-999]\n",
    "    var1 = Counter(df_train[\"Var_1\"].values[prof_inds]).most_common(1)[0][0]\n",
    "\n",
    "    # Finally return the vector\n",
    "    return np.array([gender, ever_married, sum(age)/len(age), graduated, sum(work_exp)/len(work_exp), spend_score, sum(family_size)/len(family_size), var1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54210945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ever_Married</th>\n",
       "      <th>Age</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Spending_Score</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Var_1</th>\n",
       "      <th>Segmentation</th>\n",
       "      <th>Age_group</th>\n",
       "      <th>Lonely</th>\n",
       "      <th>Binned_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462809</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462643</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>461735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>462669</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Gender  Ever_Married  Age  Graduated  Profession  Work_Experience  \\\n",
       "0  462809       0             1   22          1           5              1.0   \n",
       "1  462643       1             0   38          0           2           -999.0   \n",
       "2  466315       1             0   67          0           2              1.0   \n",
       "3  461735       0             0   67          0           7              0.0   \n",
       "4  462669       1             0   40          0           3           -999.0   \n",
       "\n",
       "   Spending_Score  Family_Size  Var_1  Segmentation  Age_group  Lonely  \\\n",
       "0               0          4.0      3             3          1       0   \n",
       "1               1          3.0      3             0          2       0   \n",
       "2               0          1.0      5             1          3       1   \n",
       "3               2          2.0      5             1          3       0   \n",
       "4               2          6.0      5             0          2       0   \n",
       "\n",
       "   Binned_ID  \n",
       "0        4.0  \n",
       "1        4.0  \n",
       "2        8.0  \n",
       "3        3.0  \n",
       "4        4.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the data\n",
    "df_train = pd.read_csv(\"Train.csv\")\n",
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "\n",
    "# Some global variables\n",
    "fill_na_val = \"JAJA\"\n",
    "seg_mapping = dict()\n",
    "\n",
    "# Convert Numeric NaNs to -999\n",
    "features = [\"Age\", \"Work_Experience\", \"Family_Size\"]\n",
    "df_train[features] = df_train[features].fillna(-999)\n",
    "df_test[features] = df_test[features].fillna(-999)\n",
    "\n",
    "# NaN in gender could mean that person is a transgender and does not want to disclose\n",
    "gender_mapping = {\"Male\": 0, \"Female\": 1}\n",
    "df_train[\"Gender\"] = df_train[\"Gender\"].fillna(2)\n",
    "df_test[\"Gender\"] = df_test[\"Gender\"].fillna(2)\n",
    "for gender, label in gender_mapping.items():\n",
    "    df_train[\"Gender\"] = df_train[\"Gender\"].replace(gender, label)\n",
    "    df_test[\"Gender\"] = df_test[\"Gender\"].replace(gender, label)\n",
    "\n",
    "# Nan in ever_married could mean that the person does not want to reveal\n",
    "# In that case it gives us extra info about the person type\n",
    "married_mapping = {\"Yes\": 0, \"No\": 1}\n",
    "df_train[\"Ever_Married\"] = df_train[\"Ever_Married\"].fillna(2)\n",
    "df_test[\"Ever_Married\"] = df_test[\"Ever_Married\"].fillna(2)\n",
    "for married, label in married_mapping.items():\n",
    "    df_train[\"Ever_Married\"] = df_train[\"Ever_Married\"].replace(married, label)\n",
    "    df_test[\"Ever_Married\"] = df_test[\"Ever_Married\"].replace(married, label)\n",
    "    \n",
    "# Nan in graduated could mean that the person has not graduate and does not want to reveal\n",
    "grad_mapping = {\"Yes\": 0, \"No\": 1}\n",
    "df_train[\"Graduated\"] = df_train[\"Graduated\"].fillna(1)\n",
    "df_test[\"Graduated\"] = df_test[\"Graduated\"].fillna(1)\n",
    "for grad, label in grad_mapping.items():\n",
    "    df_train[\"Graduated\"] = df_train[\"Graduated\"].replace(grad, label)\n",
    "    df_test[\"Graduated\"] = df_test[\"Graduated\"].replace(grad, label)\n",
    "\n",
    "# Nan in profession could mean that person is unemployed\n",
    "prof_mapping = {\"Artist\": 0, \"Doctor\": 1, \"Engineer\": 2, \"Entertainment\": 3, \"Executive\": 4, \"Healthcare\": 5, \"Homemaker\": 6, \"Lawyer\": 7, \"Marketing\": 8}\n",
    "df_train[\"Profession\"] = df_train[\"Profession\"].fillna(9)\n",
    "df_test[\"Profession\"] = df_test[\"Profession\"].fillna(9)\n",
    "for prof, label in prof_mapping.items():\n",
    "    df_train[\"Profession\"] = df_train[\"Profession\"].replace(prof, label)\n",
    "    df_test[\"Profession\"] = df_test[\"Profession\"].replace(prof, label)\n",
    "\n",
    "# Spending score is nan means it could be low\n",
    "ss_mapping = {\"Low\": 0, \"Average\": 1, \"High\": 2}\n",
    "df_train[\"Spending_Score\"] = df_train[\"Spending_Score\"].fillna(ss_mapping[\"Low\"])\n",
    "df_test[\"Spending_Score\"] = df_test[\"Spending_Score\"].fillna(ss_mapping[\"Low\"])\n",
    "for ss, label in ss_mapping.items():\n",
    "    df_train[\"Spending_Score\"] = df_train[\"Spending_Score\"].replace(ss, label)\n",
    "    df_test[\"Spending_Score\"] = df_test[\"Spending_Score\"].replace(ss, label)\n",
    "\n",
    "# NaN in Var1 is just another category\n",
    "var1_mapping = {\"Cat_1\": 0, \"Cat_2\": 1, \"Cat_3\": 2, \"Cat_4\": 3, \"Cat_5\": 4, \"Cat_6\": 5, \"Cat_7\": 6}\n",
    "df_train[\"Var_1\"] = df_train[\"Var_1\"].fillna(7)\n",
    "df_test[\"Var_1\"] = df_test[\"Var_1\"].fillna(7)\n",
    "for var1, label in var1_mapping.items():\n",
    "    df_train[\"Var_1\"] = df_train[\"Var_1\"].replace(var1, label)\n",
    "    df_test[\"Var_1\"] = df_test[\"Var_1\"].replace(var1, label)\n",
    "\n",
    "# Finally label encode segmentation\n",
    "seg_mapping = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "seg_mapping_rev = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "for seg, label in seg_mapping.items():\n",
    "    df_train[\"Segmentation\"] = df_train[\"Segmentation\"].replace(seg, label)\n",
    "\n",
    "# Based on EDA, bin the Age feature (Didn't help)\n",
    "# 1 -> young, 2 -> middle-aged, 3 -> old, 4 -> retired and old\n",
    "df_train[\"Age_group\"] = [1 if i<=33 else 2 if i>33 and i<65 else 3 if i>=65 and i<74 else 4 for i in df_train[\"Age\"].values]\n",
    "df_test[\"Age_group\"] = [1 if i<=33 else 2 if i>33 and i<65 else 3 if i>=65 and i<74 else 4 for i in df_test[\"Age\"].values]\n",
    "\n",
    "# Based on EDA, Create Lonely feature :P\n",
    "df_train[\"Lonely\"] = [1 if members<2 and age >= 60 and gender == 1 else 0 for members, age, gender in zip(df_train[\"Family_Size\"].values, df_train[\"Age\"].values, df_train[\"Gender\"].values)]\n",
    "df_test[\"Lonely\"] = [1 if members<2 and age >= 60 and gender == 1 else 0 for members, age, gender in zip(df_test[\"Family_Size\"].values, df_test[\"Age\"].values, df_test[\"Gender\"].values)]\n",
    "\n",
    "# Bin the ID column and add as feature\n",
    "est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\n",
    "df_train[\"Binned_ID\"] = est.fit_transform(np.reshape(df_train[\"ID\"].values, (-1,1)))\n",
    "df_test[\"Binned_ID\"] = est.transform(np.reshape(df_test[\"ID\"].values, (-1,1)))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a119032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns specified\n",
    "cols_to_remove = [\"ID\", \"Segmentation\"]\n",
    "df_train_dum, Y = df_train.drop(cols_to_remove, axis=1), df_train[cols_to_remove[-1]].values\n",
    "df_test_dum = df_test.drop(cols_to_remove[:-1], axis=1)\n",
    "\n",
    "# Specify which features are to be treated as Categorical features\n",
    "cat_feats = ['Gender', 'Ever_Married', 'Graduated', \"Var_1\", 'Profession', 'Age_group', 'Lonely', \"Binned_ID\"]\n",
    "cat_feats_inds = [df_train_dum.columns.get_loc(c) for c in cat_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2c6215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4684014869888476\n",
      "Score: 0.4628252788104089\n",
      "Score: 0.4634448574969021\n",
      "Score: 0.4513329200247985\n",
      "Score: 0.4463732176069436\n",
      "\n",
      "Average Score: 0.45847555218558017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# The usual\n",
    "outer_kfold = KFold(n_splits=5, random_state=27, shuffle=True)\n",
    "final_scores = list()\n",
    "for train, test in outer_kfold.split(df_train_dum):\n",
    "    x_train, x_test = df_train_dum.iloc[train].values, df_train_dum.iloc[test].values\n",
    "    y_train, y_test = Y[train], Y[test]\n",
    "    \n",
    "    model=LogisticRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    #model = LGBMClassifier(random_state=100, max_depth=3, n_estimators=200, learning_rate=0.1)\n",
    "    #model.fit(x_train, y_train, categorical_feature=cat_feats_inds)\n",
    "    preds = model.predict(x_test)\n",
    "    \n",
    "    final_scores.append(accuracy_score(y_test, preds))\n",
    "    print(\"Score:\", final_scores[-1])\n",
    "print(\"\\nAverage Score:\", np.average(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b46060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4473358116480793\n",
      "Score: 0.4392812887236679\n",
      "Score: 0.4355638166047088\n",
      "Score: 0.4426534407935524\n",
      "Score: 0.44699318040917546\n",
      "\n",
      "Average Score: 0.44236550763583676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.svm import SVC # \"Support vector classifier\" \n",
    "outer_kfold = KFold(n_splits=5, random_state=27, shuffle=True)\n",
    "final_scores = list()\n",
    "for train, test in outer_kfold.split(df_train_dum):\n",
    "    x_train, x_test = df_train_dum.iloc[train].values, df_train_dum.iloc[test].values\n",
    "    y_train, y_test = Y[train], Y[test]\n",
    "    \n",
    "    model= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  \n",
    "    model.fit(x_train, y_train)  \n",
    "     \n",
    "   \n",
    "    \n",
    "   # model = XGBClassifier(random_state=27, eval_metric=\"logloss\", max_depth=3, n_estimators=82)\n",
    "   # model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_test)\n",
    "    \n",
    "    final_scores.append(accuracy_score(y_test, preds))\n",
    "    print(\"Score:\", final_scores[-1])\n",
    "print(\"\\nAverage Score:\", np.average(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66017177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f492464ae57b4b5ea44ebc8f69f22b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition = 2 \n",
      "==================\n",
      "Final Weight 0.88\n",
      "\n",
      "Partition = 1 \n",
      "==================\n",
      "Final Weight 0.35000000000000003\n",
      "\n",
      "Score: 0.4826517967781908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a29178d7b604554b452a2e9966b8f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition = 1 \n",
      "==================\n",
      "Final Weight 0.34\n",
      "\n",
      "Partition = 2 \n",
      "==================\n",
      "Final Weight 0.9\n",
      "\n",
      "Score: 0.4739776951672863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b7fb5f01f94ffba3b8ef0ff36bf35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition = 1 \n",
      "==================\n",
      "Final Weight 0.34\n",
      "\n",
      "Partition = 2 \n",
      "==================\n",
      "Final Weight 0.62\n",
      "\n",
      "Score: 0.46592317224287483\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf52839ac6b43c082393fc33495af4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition = 1 \n",
      "==================\n",
      "Final Weight 0.77\n",
      "\n",
      "Partition = 2 \n",
      "==================\n",
      "Final Weight 0.71\n",
      "\n",
      "Score: 0.4649721016738996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a332c4177d7e46e7afd03e010f3e1729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition = 1 \n",
      "==================\n",
      "Final Weight 0.48\n",
      "\n",
      "Partition = 2 \n",
      "==================\n",
      "Final Weight 0.65\n",
      "\n",
      "Score: 0.46869187848729077\n",
      "\n",
      "Average Score: 0.4712433288699084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "outer_kfold = KFold(n_splits=5, random_state=27, shuffle=True)\n",
    "final_scores = list()\n",
    "\n",
    "# Generate the folds\n",
    "for train, test in outer_kfold.split(df_train_dum):\n",
    "    x_train, x_test = df_train_dum.iloc[train].values, df_train_dum.iloc[test].values\n",
    "    y_train, y_test = Y[train], Y[test]\n",
    "    \n",
    "    # Fit a decision tree classifier\n",
    "    # The point of this classifier is not to get best results but only to partition and map the data\n",
    "    # So the only hyper-parameter you should touch is max_leaf_nodes\n",
    "    # max_leaf_nodes not only defines the maximum leaf nodes in the decision tree but also the maximum partitions in the dataset\n",
    "    dec_tree = DecisionTreeClassifier(random_state=27, max_leaf_nodes=2)\n",
    "    dec_tree.fit(x_train, y_train)\n",
    "    # Now we know which training sample belongs to which partition\n",
    "    sample_indices = np.array(dec_tree.apply(x_train))\n",
    "    \n",
    "    # We now need to find the best weights for lightgbm and xgboost in every partition\n",
    "    # We will do this by using a KFold CV strategy\n",
    "    \n",
    "    # This dictionary will hold the weight corresponding to each partition\n",
    "    dynamic_weights = dict()\n",
    "    # For every single partition in the sample space, perform the following steps\n",
    "    for index in tqdm(list(Counter(sample_indices))):\n",
    "        indices = np.where(sample_indices==index)[0]\n",
    "        x, y = x_train[indices], y_train[indices]\n",
    "        print(\"\\nPartition =\", index, \"\\n==================\")\n",
    "        \n",
    "        # This variable consists of real numbers between 0 and 1 with a constant difference of 0.01\n",
    "        # These values correspond to weights within the average\n",
    "        weights = np.arange(0, 1, 0.01)\n",
    "        # This variable will be used to find the weight corresponding to the best score\n",
    "        scores_corr_wts = np.zeros(len(weights))\n",
    "\n",
    "        # Perform CV to get the best weights for current partition\n",
    "        kfold = KFold(n_splits=5, random_state=27, shuffle=True)\n",
    "        for inner_train, inner_test in kfold.split(x):\n",
    "            xx_train, xx_test = x[inner_train], x[inner_test]\n",
    "            yy_train, yy_test = y[inner_train], y[inner_test]\n",
    "            \n",
    "            \n",
    "            model = LGBMClassifier(random_state=100, max_depth=3, n_estimators=200, learning_rate=0.1)\n",
    "            model.fit(xx_train, yy_train)\n",
    "           #preds = model.predict(x_test)\n",
    "            preds1=model.predict_proba(xx_test)\n",
    "\n",
    "            base_model = [ (('lr',LogisticRegression())) ,('knn',KNeighborsClassifier(n_neighbors=5,metric='minkowski' ,p=2 ))]\n",
    "            meta_model=LogisticRegression()\n",
    "            stacking_model = StackingClassifier(estimators=base_model,final_estimator=meta_model, passthrough=True)\n",
    "            stacking_model.fit(xx_train,yy_train)\n",
    "            preds2=stacking_model.predict_proba(xx_test)\n",
    "            \n",
    "            # Each weight is evaluated by calculating the corresponding score\n",
    "            for i in range(len(weights)):\n",
    "                final_inner_preds = np.argmax(preds1*weights[i] + preds2*(1-weights[i]), axis=1)\n",
    "                scores_corr_wts[i]+= accuracy_score(yy_test, final_inner_preds)\n",
    "                \n",
    "        # The best weight is the one with the highest CV score\n",
    "        final_weight = weights[np.argmax(scores_corr_wts)]\n",
    "        print(\"Final Weight\", final_weight)\n",
    "        # This weight is assigned to the current partition\n",
    "        dynamic_weights[index] = final_weight\n",
    "    \n",
    "    # Once all partitions have been assigned weights, continue to create your best individual models\n",
    "    model1 = LogisticRegression()\n",
    "    model1.fit(x_train, y_train)\n",
    "\n",
    "    model2 = KNeighborsClassifier()\n",
    "    model2.fit(x_train, y_train)\n",
    "    \n",
    "    # Use the \"apply\" function offered by sklearn, again, to find out the partition id for each test sample\n",
    "    test_map = np.array(dec_tree.apply(x_test))\n",
    "    # This variable will contain all our predictions\n",
    "    final_preds = np.zeros(len(x_test))\n",
    "    for index in list(Counter(test_map)):\n",
    "        # Take all test samples that belong to current partition\n",
    "        indices = np.where(test_map == index)[0]\n",
    "        xx_test = x_test[indices]\n",
    "        # Get the weights corresponding to the current partition\n",
    "        model_weights = dynamic_weights[index]\n",
    "        # Blend the predictions using the correct dynamic weights\n",
    "        out = model1.predict_proba(xx_test)*dynamic_weights[index] + model2.predict_proba(xx_test)*(1-dynamic_weights[index])\n",
    "        # Save the prediction at the correct indices\n",
    "        final_preds[indices] = np.argmax(out, axis=1)\n",
    "    \n",
    "    final_scores.append(accuracy_score(y_test, final_preds))\n",
    "    print(\"\\nScore:\", final_scores[-1])\n",
    "print(\"\\nAverage Score:\", np.average(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed7cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
